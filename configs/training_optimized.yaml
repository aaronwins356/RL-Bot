# Optimized training configuration for RL-Bot
# This config addresses the reward plateau issue with advanced PPO features
# and better reward shaping

training:
  algorithm: "ppo"
  total_timesteps: 5000000
  batch_size: 16384  # Reduced from 32768 for more frequent updates
  n_epochs: 4  # Increased from 3 for better sample efficiency
  learning_rate: 3.0e-4  # Reduced from 8e-4 for more stable learning
  clip_range: 0.2
  gamma: 0.99  # Reduced from 0.995 for better short-term rewards
  gae_lambda: 0.95
  vf_coef: 0.5
  ent_coef: 0.02  # Increased from 0.015 for more exploration
  max_grad_norm: 0.5
  num_envs: 8
  rollout_length: 256
  
  # Advanced PPO features (CRITICAL - these were disabled before!)
  use_dynamic_lambda: true  # Adapt GAE lambda based on explained variance
  use_entropy_annealing: true  # Gradually reduce exploration
  use_clip_range_decay: true  # Decay clip range for stability
  use_reward_scaling: true  # Normalize rewards for stable learning
  
  # Dynamic GAE lambda parameters
  min_gae_lambda: 0.90  # More present-focused when learning is poor
  max_gae_lambda: 0.98  # More future-focused when learning is good
  
  # Entropy annealing parameters
  min_ent_coef: 0.001  # Minimum entropy for late training
  ent_anneal_rate: 0.9995  # Slow decay for sustained exploration
  
  # Clip range decay parameters
  clip_range_min: 0.05  # Minimum clip range
  
  # Reward scaling parameters
  reward_scale_update_freq: 1000  # Update reward stats every 1k steps
  
  # Learning rate scheduler
  lr_scheduler:
    enabled: true
    type: "cosine"  # Cosine annealing with warmup
    warmup_steps: 50000  # 50k steps warmup
    min_lr: 1.0e-5  # Minimum learning rate
  
  # Early stopping - extended patience for better convergence
  early_stop_patience: 10  # Increased for more thorough training
  
  # Self-play settings
  selfplay:
    enabled: true
    curriculum_stages: ["1v1", "1v2", "2v2"]
    opponent_update_freq: 100000  # timesteps
    elo_threshold: 50  # Lower threshold for smoother progression
  
  # Curriculum learning with automatic transitions
  curriculum:
    enabled: true
    aerial_focus: false  # Start with ground play
    auto_transition: true  # Automatically progress based on performance
    
    # Performance thresholds for stage transitions
    transition_thresholds:
      avg_reward: 0.0  # Must achieve positive average reward
      episode_length: 500  # Must maintain reasonable episode length
      win_rate: 0.3  # Must win 30% of games against current opponent
    
    # Stage definitions with progressive difficulty
    stages:
      - name: "basic_ball_chase"
        min_timesteps: 100000  # Minimum time in stage
        max_timesteps: 500000  # Auto-advance after this
        reward_weights:
          touch_ball: 1.0
          goal_scored: 2.0
          goal_conceded: -2.0
      
      - name: "positioning_basics"
        min_timesteps: 100000
        max_timesteps: 500000
        reward_weights:
          touch_ball: 0.5
          positioning: 1.0
          goal_scored: 3.0
          goal_conceded: -3.0
      
      - name: "advanced_mechanics"
        min_timesteps: 200000
        max_timesteps: 1000000
        reward_weights:
          touch_ball: 0.3
          positioning: 0.8
          aerial_touch: 1.5
          goal_scored: 5.0
          goal_conceded: -5.0
  
  # Offline training (BC pretraining)
  offline:
    enabled: false
    dataset_path: "data/telemetry_logs"
    pretrain_epochs: 5  # Reduced for faster start
    pretrain_lr: 1.0e-3
  
  # Observation and reward normalization (CRITICAL!)
  normalization:
    normalize_observations: true
    normalize_rewards: true
    clip_obs: 10.0
    clip_reward: 10.0
    reward_gamma: 0.99  # Match main gamma
    norm_update_freq: 1  # Update every step for responsive normalization
  
  # Reward shaping (enhanced)
  reward_shaping:
    enabled: true
    normalize_final_reward: true
    use_dense_rewards: true
    
    # Progressive reward complexity
    start_simple: true  # Begin with sparse rewards only
    complexity_schedule:
      - timestep: 0
        complexity: "sparse"  # Only goals and touches
      - timestep: 200000
        complexity: "basic"  # Add positioning rewards
      - timestep: 500000
        complexity: "advanced"  # Add all mechanics rewards
  
  # Enhanced evaluation
  evaluation:
    enabled: true
    eval_freq: 25000  # More frequent evaluation
    num_episodes: 10  # Episodes per evaluation
    save_replays: false  # Save best episodes
    
    # Multi-opponent evaluation
    opponents:
      - "allstar_bot"
      - "pro_bot"
      - "rookie_bot"
    
    # Early stopping based on performance
    early_stop:
      enabled: true
      patience: 5  # Evaluations without improvement
      min_delta: 10.0  # Minimum ELO improvement
  
  # League-based training (self-play pool)
  league:
    enabled: true
    pool_size: 10  # Keep 10 past checkpoints
    sampling_strategy: "prioritized"  # Sample harder opponents more
    elo_k_factor: 32  # ELO update rate
  
  # Performance optimizations (Phase 1)
  optimizations:
    # Environment vectorization
    use_subproc_vec_env: true  # Use SubprocVecEnv for true multiprocessing (Linux)
    force_dummy_vec_env: false  # Force DummyVecEnv even on Linux (for debugging)
    
    # Mixed precision training (AMP)
    use_amp: true  # Automatic Mixed Precision for GPU speedup
    amp_dtype: "float16"  # "float16" or "bfloat16" (bfloat16 requires newer GPUs)
    
    # PyTorch optimizations
    use_torch_compile: false  # PyTorch 2.0+ compile (can cause issues on some systems)
    compile_mode: "default"  # "default", "reduce-overhead", "max-autotune"
    
    # Memory optimizations
    use_pinned_memory: true  # Use pinned memory for faster CPU-GPU transfers
    num_workers: 4  # DataLoader workers for async data loading
    
    # Action repeat for faster training
    action_repeat: 1  # Repeat each action N times (1 = no repeat)
    
    # Batch inference
    batch_inference: true  # Batch observations for faster inference
    inference_batch_size: 8  # Batch size for inference (should match num_envs)
  
  # Debug mode
  debug_mode: false  # Set to true for detailed logging

network:
  architecture: "mlp"  # "mlp", "cnn", or "cnn_lstm"
  hidden_sizes: [1024, 512, 256]
  activation: "elu"
  use_lstm: false
  lstm_hidden_size: 256
  
  # Observation encoder
  encoder:
    normalize: true
    include_history: false
    history_length: 4

policy:
  type: "hybrid"  # "rule", "ml", or "hybrid"
  
  # Hybrid policy settings
  hybrid:
    use_rules_on_kickoff: true
    use_rules_on_low_confidence: true
    confidence_threshold: 0.7
    ood_detection: "entropy"  # "entropy", "mahalanobis", or "autoencoder"
    ood_threshold: 2.0
    fallback_on_saturation: true

  # Rule policy settings
  rules:
    aggressive: false
    boost_conservation: true
    safe_rotation: true

inference:
  device: "auto"  # "cpu", "cuda", or "auto" (auto-detects GPU availability)
  batch_size: 1
  frame_budget_ms: 8.0  # max time for inference (120Hz = 8.33ms per tick)

logging:
  log_dir: "logs"
  tensorboard: true
  log_interval: 1000  # Log every 1000 timesteps (more frequent)
  save_interval: 50000  # Save every 50k timesteps
  eval_interval: 25000  # Evaluate every 25k timesteps (more frequent)
  eval_num_games: 10  # Reduced from 25 for faster evaluation
  
  # Additional metrics to track
  track_metrics:
    - "episode_reward"
    - "episode_length"
    - "action_entropy"
    - "value_loss"
    - "policy_loss"
    - "explained_variance"
    - "clip_fraction"
    - "learning_rate"
    - "entropy_coef"
    - "clip_range"
    - "reward_scale"

checkpoints:
  save_dir: "checkpoints"
  keep_best_n: 5
  keep_latest: true
  save_frequency: 50000  # Checkpoint every 50k steps

telemetry:
  enabled: true
  buffer_size: 100000
  save_interval: 10000
  save_path: "data/telemetry"

notifications:
  discord:
    enabled: false
    webhook_url: null  # Set your Discord webhook URL here
    notify_on_start: true
    notify_on_checkpoint: true
    notify_on_evaluation: true
    notify_on_complete: true
    notify_on_error: true

export:
  enabled: false
  format: "torchscript"  # "torchscript", "onnx", or "raw"
  output_dir: "exported_models"
  create_rlbot_package: true
