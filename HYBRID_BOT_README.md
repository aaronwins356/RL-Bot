# RL-Bot: Hybrid Rule-Based + ML-Driven Bot

A comprehensive Rocket League bot system combining rule-based tactics with machine learning for intelligent decision-making.

## ğŸ¯ Overview

This project implements a hybrid bot architecture that:
- **Starts strong** with rule-based playbooks for kickoffs, defense, and fallback scenarios
- **Learns progressively** through reinforcement learning (PPO/SAC)
- **Adapts intelligently** by routing between rules and ML based on confidence and context
- **Trains efficiently** using both offline behavioral cloning and online self-play

## ğŸ—ï¸ Architecture

```
RL-Bot/
â”œâ”€â”€ core/                          # Core ML and training infrastructure
â”‚   â”œâ”€â”€ agents/                    # Policy implementations
â”‚   â”‚   â”œâ”€â”€ intents.py            # High-level action enums
â”‚   â”‚   â”œâ”€â”€ rule_policy.py        # Rule-based tactics
â”‚   â”‚   â”œâ”€â”€ ml_policy.py          # PyTorch model inference
â”‚   â”‚   â””â”€â”€ hybrid_policy.py      # Intelligent routing logic
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ encoder.py            # Observation â†’ feature vector
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ nets.py               # Neural architectures (MLP, CNN-LSTM, ActorCritic)
â”‚   â”‚   â””â”€â”€ ppo.py                # PPO algorithm implementation
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ buffer.py             # Experience replay buffer
â”‚   â”‚   â”œâ”€â”€ offline_dataset.py   # Telemetry replay loader
â”‚   â”‚   â”œâ”€â”€ selfplay.py           # Self-play curriculum
â”‚   â”‚   â”œâ”€â”€ train_loop.py         # Main training loop
â”‚   â”‚   â””â”€â”€ eval.py               # Elo evaluation
â”‚   â””â”€â”€ infra/
â”‚       â”œâ”€â”€ config.py             # YAML configuration management
â”‚       â”œâ”€â”€ logging.py            # Structured logging + TensorBoard
â”‚       â”œâ”€â”€ checkpoints.py        # Atomic save/load with best model promotion
â”‚       â””â”€â”€ profiler.py           # Frame time monitoring
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ test_env.py               # Mock environment for testing
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml                 # Base configuration
â”‚   â””â”€â”€ rewards.yaml              # Reward shaping configuration
â”œâ”€â”€ tests/                         # Unit tests
â”œâ”€â”€ bot.py                         # RLBot integration (existing)
â”œâ”€â”€ bot_manager.py                 # Policy loader with fallback
â”œâ”€â”€ telemetry.py                   # Telemetry logging
â””â”€â”€ main.py                        # CLI for training/inference
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/aaronwins356/RL-Bot.git
cd RL-Bot

# Install dependencies
pip install -r requirements.txt
```

### Running the Bot (Inference)

```bash
# Load hybrid policy (rule + ML)
python main.py inference --policy hybrid --model checkpoints/best_model.pt

# Use rule-based policy only
python main.py inference --policy rule

# Use ML policy only
python main.py inference --policy ml --model checkpoints/best_model.pt
```

### Training

```bash
# Train from scratch
python main.py train --config configs/base.yaml --logdir logs/run1

# Resume training from checkpoint
python main.py train --config configs/base.yaml --logdir logs/run2 --resume

# Train with custom settings
python main.py train --config configs/base.yaml --total-timesteps 5000000 --device cuda
```

### Evaluation

```bash
# Evaluate model
python main.py eval --model checkpoints/best_model.pt --config configs/base.yaml
```

### Using with RLBot

The bot integrates with RLBot using `bot.py`. To use in RLBot:

1. Copy the bot directory to your RLBot installation
2. Add the bot using `bot.cfg` in RLBot GUI
3. Start a match!

The bot will automatically load the hybrid policy and use it for decision-making.

## ğŸ§  System Components

### 1. Hybrid Policy

The `HybridPolicy` intelligently routes between rule-based and ML policies:

**Uses Rules When:**
- During kickoffs
- ML confidence is low (< 0.7)
- Out-of-distribution (OOD) observations detected
- Actuator saturation detected (controls maxed out for too long)

**Uses ML When:**
- In general play situations
- ML is confident about the decision
- Observations are in-distribution

### 2. Observation Encoder

Encodes game state into normalized feature vectors:

**Features Include:**
- Car state: position, velocity, boost, rotation, flags
- Ball state: position, velocity, predicted intercept
- Teammates: positions, velocities, relative info
- Opponents: positions, velocities, relative info
- Boost pads: nearest pads with availability
- Game state: score, time, phase (kickoff/offense/defense/neutral)

Total: 173 features (configurable)

### 3. Rule Policy

Implements hard-coded tactics:

**Behaviors:**
- **Kickoff**: Fast diagonal speedflip kickoff
- **Defense**: Shadow defense, save positioning
- **Rotation**: Safe rotation to back post
- **Boost Management**: Smart boost collection when low
- **Challenges**: Aggressive challenges when appropriate

### 4. ML Policy

PyTorch-based policy with:
- Fast inference (< 8ms target for 120Hz gameplay)
- Confidence estimation via entropy
- Actor-Critic architecture
- Optional LSTM for temporal reasoning

### 5. Training Infrastructure

**PPO Algorithm:**
- Clipped surrogate objective
- Generalized Advantage Estimation (GAE)
- Value function optimization
- Entropy regularization

**Self-Play:**
- Curriculum learning (1v1 â†’ 2v2 â†’ 3v3)
- Opponent pool with Elo ratings
- Best model promotion based on Elo threshold

**Offline Training:**
- Load telemetry from logs
- Behavioral cloning pretraining
- JSONL format for efficient loading

## âš™ï¸ Configuration

Configuration is managed via YAML files in `configs/`.

### base.yaml

```yaml
training:
  algorithm: "ppo"
  total_timesteps: 10000000
  batch_size: 4096
  learning_rate: 3.0e-4

network:
  architecture: "mlp"
  hidden_sizes: [512, 512, 256]
  activation: "relu"

policy:
  type: "hybrid"
  hybrid:
    confidence_threshold: 0.7
    use_rules_on_kickoff: true
```

### rewards.yaml

Comprehensive reward shaping:
- **Sparse**: goals, saves, demos
- **Dense**: ball interaction, positioning, boost management
- **Penalties**: double commits, whiffs, own goals
- **Advanced**: mechanical execution, team play

## ğŸ“Š Monitoring & Logging

### TensorBoard

```bash
tensorboard --logdir logs/
```

Metrics logged:
- Policy loss, value loss, entropy
- Rewards per episode
- Elo ratings
- Inference timing stats

### Telemetry

Game telemetry is automatically logged to `data/telemetry/`:
- Observations
- Actions taken
- Rewards received
- Episode information

Use for offline training or analysis.

## ğŸ§ª Testing

Run unit tests:

```bash
# All tests
pytest tests/ -v

# Specific test file
pytest tests/test_encoder.py -v

# With coverage
pytest tests/ --cov=core --cov-report=html
```

Test coverage:
- Observation encoder
- Rule policy logic
- Hybrid routing
- PPO algorithm
- Inference performance

## ğŸ® Performance

**Inference Timing:**
- Target: < 8ms per decision (120Hz gameplay)
- Actual: ~2-5ms on CPU (tested)

**Training:**
- 10M timesteps: ~12-24 hours (depending on hardware)
- Self-play: ~100k timesteps per opponent update
- Checkpoint every 10k timesteps

**Model Size:**
- Default (512-512-256): ~1.5M parameters
- Compact (256-256-128): ~500K parameters

## ğŸ”§ Customization

### Adding New Behaviors

1. Define intent in `core/agents/intents.py`
2. Implement logic in `core/agents/rule_policy.py`
3. Update routing in `core/agents/hybrid_policy.py`

### Custom Reward Shaping

Edit `configs/rewards.yaml`:

```yaml
custom_rewards:
  my_reward: 1.0
  another_reward: -0.5
```

Then implement in training environment.

### New Network Architecture

Add to `core/models/nets.py`:

```python
class MyCustomNet(nn.Module):
    def __init__(self, ...):
        # Your architecture
        pass
```

Update `configs/base.yaml` to use it.

## ğŸ“š Documentation

### Code Documentation

All modules are fully documented with docstrings. View documentation:

```python
from core.agents import HybridPolicy
help(HybridPolicy)
```

### Additional Docs

- See `SSL_UPGRADE_ANALYSIS.md` for detailed technical analysis
- See `IMPLEMENTATION_GUIDE.md` for development guidelines
- See `PROJECT_SUMMARY.md` for project overview

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional rule-based behaviors
- Advanced mechanical sequences
- SAC algorithm implementation
- Multi-agent coordination
- Opponent modeling

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ™ Credits

- **RLBot Framework**: [https://github.com/RLBot/RLBot](https://github.com/RLBot/RLBot)
- **RLGym**: [https://github.com/lucas-emery/rocket-league-gym](https://github.com/lucas-emery/rocket-league-gym)
- Original bot codebase contributors

## ğŸ“ Support

For issues or questions:
- Open an issue on GitHub
- Check existing documentation
- Review test files for usage examples

---

**Status**: âœ… Core implementation complete | ğŸš§ Training integration in progress

**Next Steps**:
1. Integrate with RLGym for actual training environment
2. Run training experiments
3. Evaluate against baseline bots
4. Fine-tune hyperparameters
5. Add advanced mechanics support
